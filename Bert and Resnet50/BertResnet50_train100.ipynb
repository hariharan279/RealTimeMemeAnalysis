{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a663d1e5-5a5f-45aa-aa59-bab541975e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harih\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from langdetect import detect_langs\n",
    "from googletrans import Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a27a65-cefb-4f66-a947-729806fc7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43cda4ba-c0ad-48ba-803d-1ac413e60ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations with data augmentation\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0c74df-df0b-4978-a822-af2360f7f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for language detection and translation\n",
    "def detect_and_translate(text):\n",
    "    try:\n",
    "        detected_langs = detect_langs(text)\n",
    "        detected_lang = detected_langs[0].lang\n",
    "        detected_prob = detected_langs[0].prob\n",
    "\n",
    "        if detected_lang != 'en' and detected_prob > 0.5:\n",
    "            translator = Translator()\n",
    "            translated_text = translator.translate(text, src=detected_lang, dest='en').text\n",
    "            return translated_text\n",
    "        else:\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in translation: {e}\")\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e40c3-975d-4497-8d44-2535e33c2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, tokenizer, label_mapping, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "            self.annotations = json.load(f)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_mapping = label_mapping\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        image_path = os.path.join(self.image_dir, annotation['image'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        text = annotation['text']  # Use your actual text processing function here\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "\n",
    "        labels = annotation.get('labels', [])\n",
    "        label_ids = [self.label_mapping[label] for label in labels]\n",
    "\n",
    "        # Convert label_ids to a fixed-size tensor, e.g., a one-hot encoded tensor\n",
    "        label_tensor = torch.zeros(len(self.label_mapping))\n",
    "        for label_id in label_ids:\n",
    "            label_tensor[label_id] = 1.0\n",
    "\n",
    "        return image, input_ids, attention_mask, label_tensor\n",
    "\n",
    "# Define the dataset and dataloader\n",
    "train_image_dir = 'C:\\\\Users\\\\harih\\\\Downloads\\\\train_images\\\\train_images'\n",
    "train_annotation_file = 'C:\\\\Users\\\\harih\\\\Downloads\\\\annotations_v2\\\\semeval2024_dev_release\\\\subtask2a\\\\train.json'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "label_mapping = {'label1': 0, 'label2': 1}  # Example label mapping\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = MemeDataset(train_image_dir, train_annotation_file, tokenizer, label_mapping, transform=image_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use binary cross-entropy for multi-label classification\n",
    "\n",
    "num_epochs = 5\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, input_ids, attention_mask, labels in train_loader:\n",
    "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642fa184-8684-45ef-8120-50001bfa0f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define the model class\n",
    "class MemeClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MemeClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Fine-tune ResNet layers\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        # Additional layers on top of ResNet50\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size + 2048, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(512, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        text_features = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        image_features = self.resnet(images)\n",
    "        features = torch.cat((text_features, image_features), dim=1)\n",
    "        \n",
    "        x = self.fc1(features)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47afa804-fe47-4dca-8b5e-417f41944150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Appeal to (Strong) Emotions': 0, 'Appeal to authority': 1, 'Appeal to fear/prejudice': 2, 'Bandwagon': 3, 'Black-and-white Fallacy/Dictatorship': 4, 'Causal Oversimplification': 5, 'Doubt': 6, 'Exaggeration/Minimisation': 7, 'Flag-waving': 8, 'Glittering generalities (Virtue)': 9, 'Loaded Language': 10, \"Misrepresentation of Someone's Position (Straw Man)\": 11, 'Name calling/Labeling': 12, 'Obfuscation, Intentional vagueness, Confusion': 13, 'Presenting Irrelevant Data (Red Herring)': 14, 'Reductio ad hitlerum': 15, 'Repetition': 16, 'Slogans': 17, 'Smears': 18, 'Thought-terminating cliché': 19, 'Transfer': 20, 'Whataboutism': 21}\n"
     ]
    }
   ],
   "source": [
    "# Load label mappings\n",
    "label_mapping = {'Appeal to (Strong) Emotions': 0, 'Appeal to authority': 1, 'Appeal to fear/prejudice': 2, 'Bandwagon': 3, 'Black-and-white Fallacy/Dictatorship': 4, 'Causal Oversimplification': 5, 'Doubt': 6, 'Exaggeration/Minimisation': 7, 'Flag-waving': 8, 'Glittering generalities (Virtue)': 9, 'Loaded Language': 10, \"Misrepresentation of Someone's Position (Straw Man)\": 11, 'Name calling/Labeling': 12, 'Obfuscation, Intentional vagueness, Confusion': 13, 'Presenting Irrelevant Data (Red Herring)': 14, 'Reductio ad hitlerum': 15, 'Repetition': 16, 'Slogans': 17, 'Smears': 18, 'Thought-terminating cliché': 19, 'Transfer': 20, 'Whataboutism': 21}  # Example label mapping\n",
    "reversed_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a56459b7-f29e-40f3-b98e-6f17a9aec04a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\cuda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\envs\\cuda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MemeClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (fc1): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=22, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = MemeClassifier(len(label_mapping))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d35f768-59f0-4c9d-b059-8164b34e38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "learning_rate = 1e-5\n",
    "batch_size = 16\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fce15a39-42d0-4739-8353-97ff64ad546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3342ea-4dc7-4f75-81b3-49c26fe0cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and dataloader\n",
    "train_image_dir = 'C:\\\\Users\\\\harih\\\\Downloads\\\\train_images\\\\train_images'\n",
    "train_annotation_file = 'C:\\\\Users\\\\harih\\\\Downloads\\\\annotations_v2\\\\semeval2024_dev_release\\\\subtask2a\\\\train.json'\n",
    "train_dataset = MemeDataset(train_image_dir, train_annotation_file, tokenizer, label_mapping, transform=image_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7aa73df-2dd3-4675-aa60-7bd0ae5cd409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harih\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.4783\n",
      "Epoch [2/100], Loss: 0.4140\n",
      "Epoch [3/100], Loss: 0.2856\n",
      "Epoch [4/100], Loss: 0.3219\n",
      "Epoch [5/100], Loss: 0.2369\n",
      "Epoch [6/100], Loss: 0.2339\n",
      "Epoch [7/100], Loss: 0.2412\n",
      "Epoch [8/100], Loss: 0.1884\n",
      "Epoch [9/100], Loss: 0.2388\n",
      "Epoch [10/100], Loss: 0.1587\n",
      "Epoch [11/100], Loss: 0.2061\n",
      "Epoch [12/100], Loss: 0.2166\n",
      "Epoch [13/100], Loss: 0.2623\n",
      "Epoch [14/100], Loss: 0.1236\n",
      "Epoch [15/100], Loss: 0.1463\n",
      "Epoch [16/100], Loss: 0.1120\n",
      "Epoch [17/100], Loss: 0.0712\n",
      "Epoch [18/100], Loss: 0.1159\n",
      "Epoch [19/100], Loss: 0.1341\n",
      "Epoch [20/100], Loss: 0.0911\n",
      "Epoch [21/100], Loss: 0.1777\n",
      "Epoch [22/100], Loss: 0.0846\n",
      "Epoch [23/100], Loss: 0.1760\n",
      "Epoch [24/100], Loss: 0.0655\n",
      "Epoch [25/100], Loss: 0.1154\n",
      "Epoch [26/100], Loss: 0.0829\n",
      "Epoch [27/100], Loss: 0.1262\n",
      "Epoch [28/100], Loss: 0.0830\n",
      "Epoch [29/100], Loss: 0.0785\n",
      "Epoch [30/100], Loss: 0.0395\n",
      "Epoch [31/100], Loss: 0.0572\n",
      "Epoch [32/100], Loss: 0.0608\n",
      "Epoch [33/100], Loss: 0.0402\n",
      "Epoch [34/100], Loss: 0.0571\n",
      "Epoch [35/100], Loss: 0.1192\n",
      "Epoch [36/100], Loss: 0.0425\n",
      "Epoch [37/100], Loss: 0.0431\n",
      "Epoch [38/100], Loss: 0.0881\n",
      "Epoch [39/100], Loss: 0.0565\n",
      "Epoch [40/100], Loss: 0.1031\n",
      "Epoch [41/100], Loss: 0.0364\n",
      "Epoch [42/100], Loss: 0.0431\n",
      "Epoch [43/100], Loss: 0.1122\n",
      "Epoch [44/100], Loss: 0.0233\n",
      "Epoch [45/100], Loss: 0.0239\n",
      "Epoch [46/100], Loss: 0.0586\n",
      "Epoch [47/100], Loss: 0.0283\n",
      "Epoch [48/100], Loss: 0.0351\n",
      "Epoch [49/100], Loss: 0.0195\n",
      "Epoch [50/100], Loss: 0.0397\n",
      "Epoch [51/100], Loss: 0.0583\n",
      "Epoch [52/100], Loss: 0.0168\n",
      "Epoch [53/100], Loss: 0.0178\n",
      "Epoch [54/100], Loss: 0.0389\n",
      "Epoch [55/100], Loss: 0.0341\n",
      "Epoch [56/100], Loss: 0.0130\n",
      "Epoch [57/100], Loss: 0.0225\n",
      "Epoch [58/100], Loss: 0.1003\n",
      "Epoch [59/100], Loss: 0.0552\n",
      "Epoch [60/100], Loss: 0.0320\n",
      "Epoch [61/100], Loss: 0.0208\n",
      "Epoch [62/100], Loss: 0.1027\n",
      "Epoch [63/100], Loss: 0.0416\n",
      "Epoch [64/100], Loss: 0.0327\n",
      "Epoch [65/100], Loss: 0.0199\n",
      "Epoch [66/100], Loss: 0.0162\n",
      "Epoch [67/100], Loss: 0.0253\n",
      "Epoch [68/100], Loss: 0.0064\n",
      "Epoch [69/100], Loss: 0.0410\n",
      "Epoch [70/100], Loss: 0.0127\n",
      "Epoch [71/100], Loss: 0.0103\n",
      "Epoch [72/100], Loss: 0.0364\n",
      "Epoch [73/100], Loss: 0.0138\n",
      "Epoch [74/100], Loss: 0.0197\n",
      "Epoch [75/100], Loss: 0.0271\n",
      "Epoch [76/100], Loss: 0.0288\n",
      "Epoch [77/100], Loss: 0.0145\n",
      "Epoch [78/100], Loss: 0.0113\n",
      "Epoch [79/100], Loss: 0.0072\n",
      "Epoch [80/100], Loss: 0.0226\n",
      "Epoch [81/100], Loss: 0.0207\n",
      "Epoch [82/100], Loss: 0.0085\n",
      "Epoch [83/100], Loss: 0.0071\n",
      "Epoch [84/100], Loss: 0.0187\n",
      "Epoch [85/100], Loss: 0.0144\n",
      "Epoch [86/100], Loss: 0.0191\n",
      "Epoch [87/100], Loss: 0.0153\n",
      "Epoch [88/100], Loss: 0.0123\n",
      "Epoch [89/100], Loss: 0.0075\n",
      "Epoch [90/100], Loss: 0.0169\n",
      "Epoch [91/100], Loss: 0.0128\n",
      "Epoch [92/100], Loss: 0.0261\n",
      "Epoch [93/100], Loss: 0.0142\n",
      "Epoch [94/100], Loss: 0.0231\n",
      "Epoch [95/100], Loss: 0.0342\n",
      "Epoch [96/100], Loss: 0.0113\n",
      "Epoch [97/100], Loss: 0.0095\n",
      "Epoch [98/100], Loss: 0.0481\n",
      "Epoch [99/100], Loss: 0.0286\n",
      "Epoch [100/100], Loss: 0.0068\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use binary cross-entropy for multi-label classification\n",
    "\n",
    "num_epochs = 100\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, input_ids, attention_mask, labels in train_loader:\n",
    "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d41af22-227e-4fb6-bff9-a801881b0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'bert_resnet_model100.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ab5ef2-2584-44f3-995c-26c04db21aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask, labels in dataloader:\n",
    "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            preds = torch.sigmoid(outputs).round().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    print(classification_report(all_labels, all_preds, target_names=reversed_label_mapping.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fee7876-7a63-48a3-9cf5-3f8787c08608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define validation dataset and dataloader\n",
    "val_image_dir = 'C:\\\\Users\\\\harih\\\\Downloads\\\\dev_images\\\\dev_images'\n",
    "val_annotation_file = 'C:\\\\Users\\\\harih\\\\Downloads\\\\annotations_v2\\\\semeval2024_dev_release\\\\subtask2a\\\\dev_unlabeled.json'\n",
    "val_dataset = MemeDataset(val_image_dir, val_annotation_file, tokenizer, label_mapping, transform=image_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddde283d-0be8-4663-9915-e2808f8dba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the validation file to get the list of IDs\n",
    "with open('C:\\\\Users\\\\harih\\\\Downloads\\\\annotations_v2\\\\semeval2024_dev_release\\\\subtask2a\\\\dev_unlabeled.json', 'r', encoding='utf-8') as f:\n",
    "    validation_data = json.load(f)\n",
    "    id_mapping = {idx: item['id'] for idx, item in enumerate(validation_data)}\n",
    "\n",
    "# Define the reversed label mapping\n",
    "reversed_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, dataloader, output_file):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, input_ids, attention_mask, labels) in enumerate(dataloader):\n",
    "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            preds = torch.sigmoid(outputs).round().cpu().numpy()\n",
    "            \n",
    "            for item_idx, (pred, label) in enumerate(zip(preds, labels.cpu().numpy())):\n",
    "                result_id = id_mapping[batch_idx * dataloader.batch_size + item_idx]\n",
    "                result = {\n",
    "                    \"id\": result_id,\n",
    "                    \"labels\": [reversed_label_mapping[i] for i, p in enumerate(pred) if p == 1.0]\n",
    "                }\n",
    "                results.append(result)\n",
    "    \n",
    "    # Save results to a JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "# Define the dataset and dataloader for validation\n",
    "val_image_dir = 'C:\\\\Users\\\\harih\\\\Downloads\\\\dev_images\\\\dev_images'\n",
    "val_annotation_file = 'C:\\\\Users\\\\harih\\\\Downloads\\\\annotations_v2\\\\semeval2024_dev_release\\\\subtask2a\\\\dev_unlabeled.json'\n",
    "val_dataset = MemeDataset(val_image_dir, val_annotation_file, tokenizer, label_mapping, transform=image_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model on validation data and save predictions\n",
    "output_file = 'resnet50_predictions.json'\n",
    "evaluate(model, val_loader, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09070c9a-09a1-46df-9a6d-d351d6a170a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.64742\tprec_h=0.70391\trec_h=0.59932\n"
     ]
    }
   ],
   "source": [
    "!python subtask_1_2a.py -g dev_gold_labels/dev_gold_labels/dev_subtask2a_en.json -p resnet50_predictions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b34def7-fe1b-472c-b7da-8e1906cf3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation file to get the list of IDs\n",
    "with open('C:\\\\Users\\\\harih\\\\Downloads\\\\test_data\\\\test_data\\\\north_macedonian\\\\mk_subtask2a_test_unlabeled.json', 'r', encoding='utf-8') as f:\n",
    "    mk_validation_data = json.load(f)\n",
    "    id_mapping = {idx: item['id'] for idx, item in enumerate(mk_validation_data)}\n",
    "mk_val_image_dir = 'C:\\\\Users\\\\harih\\\\Downloads\\\\test_images\\\\test_images\\\\subtask1_2a\\\\north_macedonian'\n",
    "mk_val_annotation_file = 'C:\\\\Users\\\\harih\\\\Downloads\\\\test_data\\\\test_data\\\\north_macedonian\\\\mk_subtask2a_test_unlabeled.json'\n",
    "mk_val_dataset = MemeDataset(mk_val_image_dir, mk_val_annotation_file, tokenizer, label_mapping, transform=image_transforms)\n",
    "mk_val_loader = DataLoader(mk_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model on validation data and save predictions\n",
    "output_file = 'mk_resnet50_predictions.json'\n",
    "evaluate(model, mk_val_loader, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fca0e56e-db65-4fdc-9673-2a3277da884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.60028\tprec_h=0.76690\trec_h=0.49314\n"
     ]
    }
   ],
   "source": [
    "!python subtask_1_2a.py -g gold_labels_ar_bg_md_version2/test_subtask2a_md.json -p mk_resnet50_predictions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da711fa2-fc2d-4863-84a3-1195a434ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation file to get the list of IDs\n",
    "with open('C:\\\\Users\\\\harih\\\\Downloads\\\\test_data\\\\test_data\\\\bulgarian\\\\bg_subtask2a_test_unlabeled.json', 'r', encoding='utf-8') as f:\n",
    "    bg_validation_data = json.load(f)\n",
    "    id_mapping = {idx: item['id'] for idx, item in enumerate(bg_validation_data)}\n",
    "bg_val_image_dir = 'C:\\\\Users\\\\harih\\\\Downloads\\\\test_images\\\\test_images\\\\subtask1_2a\\\\bulgarian'\n",
    "bg_val_annotation_file = 'C:\\\\Users\\\\harih\\\\Downloads\\\\test_data\\\\test_data\\\\bulgarian\\\\bg_subtask2a_test_unlabeled.json'\n",
    "bg_val_dataset = MemeDataset(bg_val_image_dir, bg_val_annotation_file, tokenizer, label_mapping, transform=image_transforms)\n",
    "bg_val_loader = DataLoader(bg_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model on validation data and save predictions\n",
    "output_file = 'bg_resnet50_predictions.json'\n",
    "evaluate(model, bg_val_loader, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "765ce804-d44d-42d6-806c-2b34aa9eb0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.56332\tprec_h=0.71315\trec_h=0.46552\n"
     ]
    }
   ],
   "source": [
    "!python subtask_1_2a.py -g gold_labels_ar_bg_md_version2/test_subtask2a_bg.json -p bg_resnet50_predictions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e43536c-3cdd-4f96-b1d3-77da83824553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation file to get the list of IDs\n",
    "with open('C:\\\\Users\\\\harih\\\\Downloads\\\\test_data_arabic\\\\test_data_arabic\\\\ar_subtask2a_test_unlabeled.json', 'r', encoding='utf-8') as f:\n",
    "    ar_validation_data = json.load(f)\n",
    "    id_mapping = {idx: item['id'] for idx, item in enumerate(ar_validation_data)}\n",
    "ar_val_image_dir = 'C:\\\\Users\\\\harih\\\\Downloads\\\\test_images_arabic\\\\test_images_arabic\\\\subtask2a'\n",
    "ar_val_annotation_file = 'C:\\\\Users\\\\harih\\\\Downloads\\\\test_data_arabic\\\\test_data_arabic\\\\ar_subtask2a_test_unlabeled.json'\n",
    "ar_val_dataset = MemeDataset(ar_val_image_dir, ar_val_annotation_file, tokenizer, label_mapping, transform=image_transforms)\n",
    "ar_val_loader = DataLoader(ar_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model on validation data and save predictions\n",
    "output_file = 'ar_resnet50_predictions.json'\n",
    "evaluate(model, ar_val_loader, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc9e8b2c-cf03-4b9e-81e0-74143eeb631f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.39416\tprec_h=0.52941\trec_h=0.31395\n"
     ]
    }
   ],
   "source": [
    "!python subtask_1_2a.py -g gold_labels_ar_bg_md_version2/test_subtask2a_ar.json -p ar_resnet50_predictions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aaa1032-7ba8-4f32-aa9b-9cde96952cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harih\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# Load the validation file to get the list of IDs\n",
    "def evaluate(model, dataloader, output_file):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, input_ids, attention_mask, labels) in enumerate(dataloader):\n",
    "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            preds = torch.sigmoid(outputs).round().cpu().numpy()\n",
    "            \n",
    "            for item_idx, (pred, label) in enumerate(zip(preds, labels.cpu().numpy())):\n",
    "                result_id = id_mapping[batch_idx * dataloader.batch_size + item_idx]\n",
    "                result = {\n",
    "                    \"id\": result_id,\n",
    "                    \"labels\": [reversed_label_mapping[i] for i, p in enumerate(pred) if p == 1.0]\n",
    "                }\n",
    "                results.append(result)\n",
    "    \n",
    "    # Save results to a JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "with open('C:\\\\Users\\\\harih\\\\Downloads\\\\test_data\\\\test_data\\\\english\\\\en_subtask2a_test_unlabeled.json', 'r', encoding='utf-8') as f:\n",
    "    en_validation_data = json.load(f)\n",
    "    id_mapping = {idx: item['id'] for idx, item in enumerate(en_validation_data)}\n",
    "en_val_image_dir = 'C:\\\\Users\\\\harih\\\\Downloads\\\\test_images\\\\test_images\\\\subtask1_2a\\\\english'\n",
    "en_val_annotation_file = 'C:\\\\Users\\\\harih\\\\Downloads\\\\test_data\\\\test_data\\\\english\\\\en_subtask2a_test_unlabeled.json'\n",
    "en_val_dataset = MemeDataset(en_val_image_dir, en_val_annotation_file, tokenizer, label_mapping, transform=image_transforms)\n",
    "en_val_loader = DataLoader(en_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model on validation data and save predictions\n",
    "output_file = 'en_resnet50_predictions1.json'\n",
    "evaluate(model, en_val_loader, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94edbf-658f-4de8-b1bf-4e63355a4716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71354f-045d-4ed6-ba06-25f07fdecca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c9c2a-3804-4113-bcc9-bbdd756dfa77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29546d7-48be-4ae8-b077-cd8f80fc688f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933a6f6-cfe5-41aa-9f9e-16d37a839268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
